import os
import json
import logging
import warnings
from dotenv import load_dotenv
from openai import OpenAI

# ğŸŒŸ Import ALL our physical audio processing tools
from tools.separator import separate_audio
from tools.transcriber import audio_to_midi

# Suppress noisy httpx network logs from the OpenAI SDK
logging.getLogger("httpx").setLevel(logging.WARNING)

# Securely load environment variables
load_dotenv()

# Initialize the LLM client (DeepSeek)
client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com"
)

# ==========================================
# ğŸŒŸ The Tool Registry & Schemas
# ==========================================

SEPARATE_TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "separate_audio_stems",
        "description": "Extract and separate an audio file into distinct stems: vocals, drums, bass, and other instruments. Use this when the user wants to isolate human voice or extract background music.",
        "parameters": {
            "type": "object",
            "properties": {
                "input_file_path": {
                    "type": "string",
                    "description": "The relative or absolute file path to the target audio file."
                }
            },
            "required": ["input_file_path"]
        }
    }
}

TRANSCRIBE_TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "audio_to_midi",
        "description": "Convert a separated audio track (like piano, guitar, vocal, or 'other.wav') into a MIDI file. MUST be used AFTER separating the audio if the user wants MIDI from a mixed song.",
        "parameters": {
            "type": "object",
            "properties": {
                "input_file_path": {
                    "type": "string",
                    "description": "The path to the SPECIFIC audio file to transcribe (e.g., the output path of 'other.wav' or 'vocals.wav' generated by separate_audio_stems)."
                }
            },
            "required": ["input_file_path"]
        }
    }
}

# Map string names to the actual Python functions
AVAILABLE_TOOLS = {
    "separate_audio_stems": separate_audio,
    "audio_to_midi": audio_to_midi
}

# ==========================================
# ğŸŒŸ The Core Agentic Loop
# ==========================================

def run_agent_workflow(user_prompt: str) -> str:
    """
    The True Agent Execution Loop: Supports chaining multiple tools sequentially.
    """
    print(f"\n[User] {user_prompt}\n")
    print("-" * 50)
    
    # ğŸŒŸ CRITICAL: The System Prompt instructs the LLM how to chain tools
    system_prompt = (
        "You are Music-Agent, a professional AI audio processing assistant. "
        "You have tools to separate audio and convert audio to MIDI. "
        "If the user asks to extract accompaniment and convert it to MIDI, you MUST do it in steps: "
        "1. Call separate_audio_stems. "
        "2. Read the tool output to find the exact path for 'other.wav' (the accompaniment). "
        "3. Call audio_to_midi using that specific path. "
        "Finally, summarize all generated file paths for the user."
    )
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # The ReAct (Reasoning and Acting) Loop - Allow up to 5 turns of thought
    max_turns = 5
    for turn in range(max_turns):
        print(f"ğŸ§  [Agent Brain - Turn {turn + 1}] Thinking...")
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,
            tools=[SEPARATE_TOOL_SCHEMA, TRANSCRIBE_TOOL_SCHEMA], # ğŸŒŸ Added both tools!
            tool_choice="auto"
        )

        response_message = response.choices[0].message
        messages.append(response_message) # Save thought to memory

        # Did the LLM decide to use a tool?
        if response_message.tool_calls:
            for tool_call in response_message.tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                print(f"ğŸ› ï¸  [Execution] Triggered tool: '{function_name}'")
                print(f"   ğŸ“¦ Args: {function_args}")
                
                function_to_call = AVAILABLE_TOOLS.get(function_name)
                
                if function_to_call:
                    print("âš™ï¸  [Execution] Running physics code... Please wait.")
                    tool_result = function_to_call(input_file_path=function_args.get("input_file_path"))
                    print(f"âœ… [Execution] '{function_name}' finished.")
                    
                    # Feed the result back to the LLM
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "name": function_name,
                        "content": json.dumps(tool_result)
                    })
                else:
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "name": function_name,
                        "content": json.dumps({"error": "Tool not found"})
                    })
            # ğŸŒŸ Note: The loop continues! The LLM will now read the tool output and think again.
            
        else:
            # If no tools were called, the LLM has finished its task and generated standard text!
            final_answer = response_message.content
            print("\nâœ¨ [Final Answer]")
            print(final_answer)
            return final_answer
            
    return "âŒ Error: Agent reached maximum turns without completing the task."

# ==========================================
# Local Testing Block
# ==========================================
if __name__ == "__main__":
    # ğŸŒŸ æµ‹è¯•ç»ˆæé“¾å¼è°ƒç”¨ï¼
    run_agent_workflow("è¯·æŠŠ test.mp3 çš„ä¼´å¥æå–å‡ºæ¥ï¼Œç„¶åæŠŠå®ƒè½¬æ¢æˆ MIDI ä¹è°±ç»™æˆ‘ã€‚")